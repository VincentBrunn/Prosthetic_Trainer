# **Open-Source Hand Gesture Recognition with KNN**
### **Train Your Own Hand Gesture Model for Custom Applications**  

ğŸš€ **This project provides an easy-to-use pipeline for training a KNN-based hand gesture recognition model.**  
The goal is to allow users to **define, train, and recognize** their own hand gestures without requiring deep learning expertise.  
This will later be used for **prosthetic control**, where users can customize gestures for their prosthetic hands.

---

## **ğŸ”¹ Features**
âœ… **Train a custom hand gesture model** using your own hand movements  
âœ… **Uses MediaPipe for real-time hand tracking**  
âœ… **K-Nearest Neighbors (KNN) classifier for lightweight gesture recognition**  
âœ… **Open-source and adaptable for different use cases**  
âœ… **Easily expand gesture sets for various applications**  

---

## **ğŸ“Œ Future Applications**
This system is designed as **a foundation for a future prosthetic control system**.  
- Users can **train their own gestures**, allowing for **personalized prosthetic hand control**.  
- The trained model can be **exported to hardware** for **gesture-to-movement conversion**.  
- This project enables **rapid prototyping** for **robotics, assistive devices, and hands-free control systems**.  

---

## **ğŸš€ Getting Started**
### **ğŸ”¹ Installation**
Make sure you have **Python 3.8+** installed.  

1ï¸âƒ£ **Clone this repository**:
```bash
git clone https://github.com/yourusername/Hand-Gesture-KNN.git
cd Hand-Gesture-KNN
```

2ï¸âƒ£ **Set up a virtual environment (recommended)**:
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows
```

3ï¸âƒ£ **Install dependencies**:
```bash
pip install -r requirements.txt
```

---

## **ğŸ–ï¸ Training Your Own Gestures**
1ï¸âƒ£ **Run the Gesture Data Collection Script**  
This will allow you to record your own hand gestures.  

```bash
python gesture_data_collection.py
```
ğŸ¥ **How it works:**  
- The webcam will open.  
- Perform your gesture and press a key to label it:
  - **`T`** â†’ Save "Thumbs Up"  
  - **`F`** â†’ Save "Fist"  
  - **`O`** â†’ Save "Open Palm"  
- Press **`Q`** to exit.  
- The recorded gestures are saved in **`gesture_data.pkl`**.

---

2ï¸âƒ£ **Train Your Gesture Model**  
After collecting data, train the KNN model with:
```bash
python gesture_trainer.py
```
ğŸ¯ **What happens here?**  
- Your recorded hand keypoints are loaded from `gesture_data.pkl`.  
- A **KNN classifier** is trained based on your labeled gestures.  
- The trained model is saved as **`gesture_classifier.pkl`**.

---

3ï¸âƒ£ **Test the Trained Model (Live Gesture Prediction)**
Run:
```bash
python Webcam_Gesture.py
```
ğŸ¥ **This will open the webcam and start recognizing gestures in real time.**  
Youâ€™ll see predicted gestures displayed on the screen.

---

## **ğŸ› ï¸ How It Works**
### **ğŸ”¹ Key Technologies**
- **MediaPipe Hands** â†’ Real-time hand landmark detection
- **OpenCV** â†’ Webcam processing & visualization
- **Scikit-Learn (KNN)** â†’ Lightweight machine learning for classifying gestures

### **ğŸ”¹ How Gestures Are Recognized**
1. The webcam **captures your hand position**.
2. MediaPipe extracts **21 key landmarks** (fingertips, joints, palm center).
3. A KNN classifier **matches the keypoints** to your trained gestures.
4. The predicted gesture is **displayed on the screen**.

---

## **ğŸ¯ Adapting This for Prosthetic Control**
This project is designed for **future prosthetic applications**.  
ğŸ‘‹ **How will it work?**
1. **Users train their own gestures** to match desired prosthetic movements.
2. The system **translates gestures into control signals**.
3. The trained model is **integrated into a prosthetic handâ€™s software**, mapping gestures to physical movements.

ğŸ“Œ **Future Steps:**
- Convert gesture predictions into **serial data for prosthetic communication**.
- Implement an **MQTT network** for real-time robotic hand control.
- Optimize the classifier for **faster and more accurate recognition**.

---

## **ğŸ› ï¸ Customizing for Your Own Gestures**
Want to add a new gesture?  
1. **Edit `gesture_data_collection.py`** and add a new key in `gesture_data`:
   ```python
   gesture_data = {
       "thumbs_up": [],
       "fist": [],
       "open_palm": [],
       "peace_sign": []  # ğŸ‘ˆ Add a new gesture
   }
   ```
2. Modify the key press mapping:
   ```python
   if key == ord("p"):  # Press "P" for peace sign
       gesture_data["peace_sign"].append(keypoints)
       print("Saved Peace Sign Gesture")
   ```
3. Train the model again:
   ```bash
   python gesture_trainer.py
   ```
4. Run live recognition:
   ```bash
   python Webcam_Gesture.py
   ```

---

## **ğŸ“Œ File Structure**
```
Hand-Gesture-KNN/
â”‚â”€â”€ gesture_data_collection.py  # Collects hand gestures via webcam
â”‚â”€â”€ gesture_trainer.py          # Trains the KNN model
â”‚â”€â”€ Webcam_Gesture.py           # Runs real-time gesture recognition
â”‚â”€â”€ gesture_data.pkl            # Stores recorded gesture data
â”‚â”€â”€ gesture_classifier.pkl      # Trained KNN model
â”‚â”€â”€ requirements.txt            # Dependencies list
â”‚â”€â”€ README.md                   # Documentation
```

---

## **ğŸ¤ Contributing**
This is an **open-source project**!  
ğŸ”¹ Feel free to **add new gestures, improve accuracy, or integrate with hardware**.  
ğŸ”¹ If you create an adaptation (e.g., **robotic hand control**), consider contributing back!

---

## **ğŸ“„ License**
This project is licensed under the **MIT License**. You are free to modify and use it in your own projects.

---

## **ğŸ“¬ Contact & Feedback**
Got ideas? Want to contribute? Open an issue or pull request!  

ğŸ“§ **Email:** [Your Email]  
ğŸ’¡ **GitHub:** [Your GitHub Profile]  

ğŸš€ **Happy coding, and letâ€™s build the future of assistive technology together!** ğŸš€

---

### **ğŸ”¥ Summary**
âœ… **Fully customizable gesture training system**  
âœ… **Trains and classifies gestures using KNN**  
âœ… **Designed for future prosthetic hand control**  
âœ… **Open-source and easy to extend** 
